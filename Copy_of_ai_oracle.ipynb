{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevenbowler/CustomerResponseChatBot/blob/master/Copy_of_ai_oracle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AI-Oracle, by [Matt Shumer](https://twitter.com/mattshumer_)"
      ],
      "metadata": {
        "id": "_E_K_PU3qsqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "AZikgJ2tdYvd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a9e7476-aeab-4704-be8e-e652b884caba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.16.2-py3-none-any.whl (267 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/267.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m235.5/267.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "KLKKbbbO6Rvr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ANTHROPIC_API_KEY = userdata.get(\"ANTHROPIC_API_KEY\")  # Replace with your Anthropic API key\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\") # Replace with your OpenAI API key\n",
        "PPLX_API_KEY = userdata.get(\"PERPLEXITY_API_KEY\") # Replace with your Perplexity API key"
      ],
      "metadata": {
        "id": "x48QfV6jdLdz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "BGjpZBCYcUlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_claude_step_1(prompt, model=\"claude-3-opus-20240229\", max_tokens=2000, temperature=0.1): # using 0.1 for precision during the pre-answer phase\n",
        "    headers = {\n",
        "        \"x-api-key\": ANTHROPIC_API_KEY,\n",
        "        \"anthropic-version\": \"2023-06-01\",\n",
        "        \"content-type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"system\": \"You are a world-class expert across every topic. Answer with incredibly accurate and useful responses.\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "    }\n",
        "    response = requests.post(\"https://api.anthropic.com/v1/messages\", headers=headers, json=data)\n",
        "    print(response.json())\n",
        "    return response.json()['content'][0]['text']\n",
        "\n",
        "\n",
        "\n",
        "def generate_gpt_step_1(prompt, model=\"gpt-4\", max_tokens=2000, temperature=0.1): # using 0.1 for precision during the pre-answer phase\n",
        "  response = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a world-class expert across every topic. Answer with incredibly accurate and useful responses.\"\n",
        "      },\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt,\n",
        "      }\n",
        "    ],\n",
        "    max_tokens=max_tokens,\n",
        "    temperature=temperature,\n",
        "  )\n",
        "\n",
        "  return response.choices[0].message.content\n",
        "\n",
        "\n",
        "\n",
        "def generate_perplexity_step_1(prompt, model=\"pplx-70b-online\", max_tokens=2000, temperature=0.1): # using 0.1 for precision during the pre-answer phase\n",
        "  payload = {\n",
        "      \"model\": model,\n",
        "      \"messages\": [\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": \"Be precise.\"\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt,\n",
        "          }\n",
        "      ]\n",
        "  }\n",
        "  headers = {\n",
        "      \"accept\": \"application/json\",\n",
        "      \"content-type\": \"application/json\",\n",
        "      \"Authorization\": f\"Bearer {PPLX_API_KEY}\"\n",
        "  }\n",
        "\n",
        "  response = requests.post(\"https://api.perplexity.ai/chat/completions\", json=payload, headers=headers)\n",
        "  print(response.json())\n",
        "\n",
        "  return response.json()['choices'][0]['message']['content']\n",
        "\n",
        "\n",
        "def generate_claude_step_2(question, claude_answer, gpt_answer, pplx_answer, model=\"claude-3-opus-20240229\", max_tokens=2000, temperature=0.6): # using 0.6 for flowiness during the answer phase\n",
        "    prompt = f\"\"\"Here is the user's question:\n",
        "<user_question>\n",
        "{question}\n",
        "</user_question>\n",
        "\n",
        "<ai_answers_to_inform_your_response>\n",
        "GPT-4, which is strong at reasoning, responded with:\n",
        "<gpt_4_response>\n",
        "{gpt_answer}\n",
        "</gpt_4_response>\n",
        "\n",
        "Claude 3, which is strong at reasoning and has a great personality, responded with:\n",
        "<claude_3_response>\n",
        "{claude_answer}\n",
        "</claude_3_response>\n",
        "\n",
        "PPLX AI, which is weaker at reasoning but has access to up-to-date information from the internet, responded with:\n",
        "<pplx_response>\n",
        "{pplx_answer}\n",
        "</pplx_response>\n",
        "</ai_answers_to_inform_your_response>\n",
        "\n",
        "Again, the user's question is:\n",
        "<user_question>\n",
        "{question}\n",
        "</user_question>\n",
        "\n",
        "Use all of these AI answers to inform your final answer. Now, answer the user's question perfectly.\"\"\"\n",
        "\n",
        "    headers = {\n",
        "        \"x-api-key\": ANTHROPIC_API_KEY,\n",
        "        \"anthropic-version\": \"2023-06-01\",\n",
        "        \"content-type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"system\": \"The user has asked a question, and we've asked three different LLMs to response. You will take all of their outputs, and combine their strengths and mitigate their mistakes into a final, perfect answer.\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "    }\n",
        "    response = requests.post(\"https://api.anthropic.com/v1/messages\", headers=headers, json=data)\n",
        "    print(response.json())\n",
        "    return response.json()['content'][0]['text']\n",
        "\n",
        "def generate_response(question):\n",
        "\n",
        "  claude = generate_claude_step_1(question)\n",
        "  gpt = generate_gpt_step_1(question)\n",
        "  pplx = generate_perplexity_step_1(question)\n",
        "\n",
        "  final = generate_claude_step_2(question, claude, gpt, pplx)\n",
        "\n",
        "  return final, claude, gpt, pplx"
      ],
      "metadata": {
        "id": "fouhWkGPcb2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is today's news?\"\n",
        "\n",
        "response = generate_response(question)\n",
        "\n",
        "print('\\n\\n', response[0])"
      ],
      "metadata": {
        "id": "hLwXQ04Ldg2c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}